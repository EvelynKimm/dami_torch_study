{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage in PyTorch\n",
    "\n",
    "3채널짜리 64 * 64 이미지 하나를 표현하는 무작위 데이터 텐서 생성 -> 이에 상응하는 label을 무작위 값으로 초기화 \n",
    "\n",
    "+ 3채널 = 이미지의 색상 채널. 일반적으로 3채널은 RGB 색상 모델을 의미\n",
    "\n",
    "미리 학습된 모델의 label은 (1,1000)의 모양을 가짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "data = torch.rand(1, 3, 64, 64)\n",
    "labels = torch.rand(1, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward Pass**\n",
    "\n",
    "model에 input data를 넣어줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating Loss**\n",
    "\n",
    "'모델 예측값 - 정답'을 통해 오차를 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (prediction - labels).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backward Pass**\n",
    "\n",
    "계산한 오차를 역전파\n",
    "\n",
    "오차 텐서에 .backward() 를 호출해 역전파 시작 \n",
    "\n",
    "-> Autograd(파이토치의 자동 미분 엔진)가 파라미터의 .grad 속성에 모델의 각 파라미터에 대한 변화도(gradient)를 계산하고 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimizer**\n",
    "\n",
    "optimizer를 불러와 모델의 파라미터를 등록해줌.\n",
    "\n",
    "learning rate = 0.01\n",
    "\n",
    "momentum = 0.9\n",
    "\n",
    "momentum은 SGD에서 학습을 빠르고 안정적으로 진행하기 위해 사용하는 기술. \n",
    "\n",
    "기본 SGD는 현재의 기울기만을 기반으로 파라미터를 업데이터하지만 momentum을 사용하면 이전 단계의 기울기를 일정 비율로 고려하여 업데이트에 반영"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step**\n",
    "\n",
    "step을 호출하여 경사하강법을 시작. 옵티마이저는 .grad에 저장된 변화도에 따라 각 파라미터를 조정."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differentiaton in Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "숫자 뒤에 \".\"찍어주는 이유 : float 자료형을 명시적으로 나타내기 위해서\n",
    "\n",
    "\".\"을 찍지 않으면 int형으로 생성되는데, ML/DL 모델에서는 주로 float형을 사용한다. 정밀한 계산, 미분을 위해 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "새로운 텐서 Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 3*a**3 - b**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a, b가 모두 신경망(NN)의 파라미터이고, Q는 오차이다.\n",
    "\n",
    "1. Q에 대해 .backward()를 호출할 때, autograd는 이러한 변화도를 계산하고 이를 각 텐서의 .grad 속성에 저장.\n",
    "\n",
    "2-1. Q가 단일 스칼라 값인 경우, backward()를 그대로 호출해주면 된다.\n",
    "2-2. Q가 단일 스칼라 값이 아니라 벡터인 경우에는 방향을 알려주기 위해 Q.backward()에 gradient 인자를 명시적으로 전달해야 한다. gradient는 Q와 같은 모양의 텐서로, Q 자기 자신에 대한 gradient를 나타낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연산 그래프(Computational Graph)\n",
    "\n",
    "파이토치의 Autograd에서 사용하는 연산 그래프 \n",
    "\n",
    "+ Autograd는 텐서 연산을 추적하여 기울기를 자동으로 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Directed Acyclic Graph(DAG)**\n",
    "\n",
    "파이토치는 DAG 형태로 연산 기록을 저장. 이 그래프는 텐서와 연산으로 구성.\n",
    "\n",
    "리프 노드는 입력 텐서(파라미터 등), 루트 노드는 출력 텐서(손실 값 등)\n",
    "\n",
    "**순전파**\n",
    "\n",
    "Autograd는 순전파 시, 연산 실행하여 결과 텐서를 저장. 동시에 각 연산의 기울기 함수를 DAG에 저장. 이 기울기 함수는 이후 역전파에 사용됨.\n",
    "\n",
    "**역전파**\n",
    "\n",
    "루트 노드에 .backward()메서드를 호출하면 역전파 시작\n",
    "\n",
    "Autograd는 이 루트에서 출발하여 그래프를 따라 리프 노드까지 이동하며 각 노드의 기울기를 계산 (using chain rule)\n",
    "\n",
    "각 텐서의 .grad 속성에 계산된 기울기를 누적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5, 5)\n",
    "y = torch.rand(5, 5)\n",
    "z = torch.rand((5, 5), requires_grad=True)\n",
    "\n",
    "a = x + y\n",
    "print(f\"Does `a` require gradients?: {a.requires_grad}\")\n",
    "b = x + z\n",
    "print(f\"Does `b` require gradients?: {b.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "model = resnet18(weights = ResNet18_Weights.DEFAULT)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(512, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
